{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Project Day 3: Use LSTM or fine-tune BERT for the Product Safety Dataset\n",
    "\n",
    "We continue to work with the final project dataset. This time you can work with [LSTM](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html) (its use is similar to RNN) or fine-tune a BERT model. Be careful with the BERT approach as it may take a long time. You will again predict the __human_tag__ field of the dataset.\n",
    "\n",
    "Use the notebooks from the class and implement the model, train and test with the corresponding datasets.\n",
    "You can follow these steps:\n",
    "1. Read training-test data (Given)\n",
    "2. Train a classifier (Implement)\n",
    "3. Make predictions on your test dataset (Implement)\n",
    "4. Write your test predictions to a CSV file (Given)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p36/bin/pip\", line 5, in <module>\n",
      "    from pip._internal.cli.main import main\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/pip/_internal/cli/main.py\", line 9, in <module>\n",
      "    from pip._internal.cli.autocompletion import autocomplete\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/pip/_internal/cli/autocompletion.py\", line 10, in <module>\n",
      "    from pip._internal.cli.main_parser import create_main_parser\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/pip/_internal/cli/main_parser.py\", line 8, in <module>\n",
      "    from pip._internal.cli import cmdoptions\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/pip/_internal/cli/cmdoptions.py\", line 23, in <module>\n",
      "    from pip._internal.cli.parser import ConfigOptionParser\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/pip/_internal/cli/parser.py\", line 12, in <module>\n",
      "    from pip._internal.configuration import Configuration, ConfigurationError\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/pip/_internal/configuration.py\", line 20, in <module>\n",
      "    from pip._internal.exceptions import (\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/pip/_internal/exceptions.py\", line 7, in <module>\n",
      "    from pip._vendor.pkg_resources import Distribution\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/pip/_vendor/pkg_resources/__init__.py\", line 84, in <module>\n",
      "    __import__('pip._vendor.packaging.requirements')\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/pip/_vendor/packaging/requirements.py\", line 10, in <module>\n",
      "    from pip._vendor.pyparsing import (  # noqa\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/pip/_vendor/pyparsing.py\", line 5672, in <module>\n",
      "    _escapedPunc = Word(_bslash, r\"\\[]-*.$+^?()~ \", exact=2).setParseAction(lambda s, l, t: t[0][1])\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/pip/_vendor/pyparsing.py\", line 1563, in setParseAction\n",
      "    self.parseAction = list(map(_trim_arity, list(fns)))\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/pip/_vendor/pyparsing.py\", line 1310, in _trim_arity\n",
      "    this_line = extract_stack(limit=2)[-1]\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/pip/_vendor/pyparsing.py\", line 1294, in extract_stack\n",
      "    frame_summary = traceback.extract_stack(limit=-offset + limit - 1)[offset]\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/traceback.py\", line 211, in extract_stack\n",
      "    stack = StackSummary.extract(walk_stack(f), limit=limit)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/traceback.py\", line 364, in extract\n",
      "    f.line\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/traceback.py\", line 286, in line\n",
      "    self._line = linecache.getline(self.filename, self.lineno).strip()\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/linecache.py\", line 16, in getline\n",
      "    lines = getlines(filename, module_globals)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/linecache.py\", line 47, in getlines\n",
      "    return updatecache(filename, module_globals)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/linecache.py\", line 137, in updatecache\n",
      "    lines = fp.readlines()\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/codecs.py\", line 318, in decode\n",
      "    def decode(self, input, final=False):\n",
      "KeyboardInterrupt\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p36/bin/pip\", line 5, in <module>\n",
      "    from pip._internal.cli.main import main\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/pip/_internal/cli/main.py\", line 9, in <module>\n",
      "    from pip._internal.cli.autocompletion import autocomplete\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/pip/_internal/cli/autocompletion.py\", line 10, in <module>\n",
      "    from pip._internal.cli.main_parser import create_main_parser\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/pip/_internal/cli/main_parser.py\", line 8, in <module>\n",
      "    from pip._internal.cli import cmdoptions\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/pip/_internal/cli/cmdoptions.py\", line 23, in <module>\n",
      "    from pip._internal.cli.parser import ConfigOptionParser\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/pip/_internal/cli/parser.py\", line 12, in <module>\n",
      "    from pip._internal.configuration import Configuration, ConfigurationError\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/pip/_internal/configuration.py\", line 20, in <module>\n",
      "    from pip._internal.exceptions import (\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/pip/_internal/exceptions.py\", line 7, in <module>\n",
      "    from pip._vendor.pkg_resources import Distribution\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/pip/_vendor/pkg_resources/__init__.py\", line 84, in <module>\n",
      "    __import__('pip._vendor.packaging.requirements')\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/pip/_vendor/packaging/requirements.py\", line 10, in <module>\n",
      "    from pip._vendor.pyparsing import (  # noqa\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/pip/_vendor/pyparsing.py\", line 5672, in <module>\n",
      "    _escapedPunc = Word(_bslash, r\"\\[]-*.$+^?()~ \", exact=2).setParseAction(lambda s, l, t: t[0][1])\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/pip/_vendor/pyparsing.py\", line 1563, in setParseAction\n",
      "    self.parseAction = list(map(_trim_arity, list(fns)))\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/pip/_vendor/pyparsing.py\", line 1310, in _trim_arity\n",
      "    this_line = extract_stack(limit=2)[-1]\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/pip/_vendor/pyparsing.py\", line 1294, in extract_stack\n",
      "    frame_summary = traceback.extract_stack(limit=-offset + limit - 1)[offset]\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/traceback.py\", line 211, in extract_stack\n",
      "    stack = StackSummary.extract(walk_stack(f), limit=limit)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/traceback.py\", line 364, in extract\n",
      "    f.line\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/traceback.py\", line 286, in line\n",
      "    self._line = linecache.getline(self.filename, self.lineno).strip()\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/linecache.py\", line 16, in getline\n",
      "    lines = getlines(filename, module_globals)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/linecache.py\", line 47, in getlines\n",
      "    return updatecache(filename, module_globals)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/linecache.py\", line 137, in updatecache\n",
      "    lines = fp.readlines()\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/codecs.py\", line 318, in decode\n",
      "    def decode(self, input, final=False):\n",
      "KeyboardInterrupt\n",
      "Requirement already satisfied: pytorch-pretrained-bert in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (0.6.2)\n",
      "Requirement already satisfied: pytorch-nlp in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (0.5.0)\n",
      "Requirement already satisfied: torch>=0.4.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from pytorch-pretrained-bert) (1.8.1)\n",
      "Requirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from pytorch-pretrained-bert) (1.19.5)\n",
      "Requirement already satisfied: boto3 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from pytorch-pretrained-bert) (1.23.10)\n",
      "Requirement already satisfied: tqdm in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from pytorch-pretrained-bert) (4.64.1)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from pytorch-pretrained-bert) (2.25.1)\n",
      "Requirement already satisfied: regex in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from pytorch-pretrained-bert) (2021.4.4)\n",
      "Requirement already satisfied: typing-extensions in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from torch>=0.4.1->pytorch-pretrained-bert) (3.7.4.3)\n",
      "Requirement already satisfied: dataclasses in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from torch>=0.4.1->pytorch-pretrained-bert) (0.8)\n",
      "Requirement already satisfied: pytorch-pretrained-bert in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (0.6.2)\n",
      "Requirement already satisfied: pytorch-nlp in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (0.5.0)\n",
      "Requirement already satisfied: torch>=0.4.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from pytorch-pretrained-bert) (1.8.1)\n",
      "Requirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from pytorch-pretrained-bert) (1.19.5)\n",
      "Requirement already satisfied: boto3 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from pytorch-pretrained-bert) (1.23.10)\n",
      "Requirement already satisfied: tqdm in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from pytorch-pretrained-bert) (4.64.1)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from pytorch-pretrained-bert) (2.25.1)\n",
      "Requirement already satisfied: regex in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from pytorch-pretrained-bert) (2021.4.4)\n",
      "Requirement already satisfied: typing-extensions in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from torch>=0.4.1->pytorch-pretrained-bert) (3.7.4.3)\n",
      "Requirement already satisfied: dataclasses in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from torch>=0.4.1->pytorch-pretrained-bert) (0.8)\n",
      "Requirement already satisfied: botocore<1.27.0,>=1.26.10 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from boto3->pytorch-pretrained-bert) (1.26.10)\n",
      "Requirement already satisfied: s3transfer<0.6.0,>=0.5.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from boto3->pytorch-pretrained-bert) (0.5.0)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from boto3->pytorch-pretrained-bert) (0.10.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests->pytorch-pretrained-bert) (2021.5.30)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests->pytorch-pretrained-bert) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests->pytorch-pretrained-bert) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests->pytorch-pretrained-bert) (1.26.8)\n",
      "Requirement already satisfied: importlib-resources in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tqdm->pytorch-pretrained-bert) (5.4.0)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from botocore<1.27.0,>=1.26.10->boto3->pytorch-pretrained-bert) (2.8.1)\n",
      "Requirement already satisfied: botocore<1.27.0,>=1.26.10 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from boto3->pytorch-pretrained-bert) (1.26.10)\n",
      "Requirement already satisfied: s3transfer<0.6.0,>=0.5.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from boto3->pytorch-pretrained-bert) (0.5.0)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from boto3->pytorch-pretrained-bert) (0.10.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests->pytorch-pretrained-bert) (2021.5.30)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests->pytorch-pretrained-bert) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests->pytorch-pretrained-bert) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests->pytorch-pretrained-bert) (1.26.8)\n",
      "Requirement already satisfied: importlib-resources in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tqdm->pytorch-pretrained-bert) (5.4.0)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from botocore<1.27.0,>=1.26.10->boto3->pytorch-pretrained-bert) (2.8.1)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from importlib-resources->tqdm->pytorch-pretrained-bert) (3.4.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.27.0,>=1.26.10->boto3->pytorch-pretrained-bert) (1.15.0)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from importlib-resources->tqdm->pytorch-pretrained-bert) (3.4.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.27.0,>=1.26.10->boto3->pytorch-pretrained-bert) (1.15.0)\n"
     ]
    }
   ],
   "source": [
    "# Upgrade dependencies\n",
    "!pip install -r ../../requirements.txt\n",
    "!pip install pytorch-pretrained-bert pytorch-nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import os\n",
    "from os import path\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Reading the dataset\n",
    "\n",
    "We will use the __pandas__ library to read our dataset. Let's first download the files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Training data:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('../../data/final_project/training.csv', encoding='utf-8', header=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Test data:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_text(X):\n",
    "    X.text=X.text.apply(str)\n",
    "    X.star_rating=X.star_rating.apply(str)\n",
    "    X.title=X.title.apply(str)\n",
    "    X['concat']=X[['text','star_rating', 'title']].agg(' '.join, axis=1)    \n",
    "    X['concat']= \"[CLS] \" + X['concat'] + \" [SEP]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>doc_id</th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "      <th>star_rating</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>62199</td>\n",
       "      <td>15449606311</td>\n",
       "      <td>Quality of material is great, however, the bac...</td>\n",
       "      <td>3/7/2018 19:47</td>\n",
       "      <td>3</td>\n",
       "      <td>great backpack with strange fit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>76123</td>\n",
       "      <td>15307152511</td>\n",
       "      <td>The product was okay but wasn't refined campho...</td>\n",
       "      <td>43135.875</td>\n",
       "      <td>2</td>\n",
       "      <td>Not refined</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>78742</td>\n",
       "      <td>12762748321</td>\n",
       "      <td>I normally read the reviews before buying some...</td>\n",
       "      <td>42997.37708</td>\n",
       "      <td>1</td>\n",
       "      <td>Doesnt work, wouldnt recommend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>64010</td>\n",
       "      <td>15936405041</td>\n",
       "      <td>These pads are completely worthless. The light...</td>\n",
       "      <td>43313.25417</td>\n",
       "      <td>1</td>\n",
       "      <td>The lighter colored side of the pads smells li...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17058</td>\n",
       "      <td>13596875291</td>\n",
       "      <td>The saw works great but the blade oiler does n...</td>\n",
       "      <td>12/5/2017 20:17</td>\n",
       "      <td>2</td>\n",
       "      <td>The saw works great but the blade oiler does n...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      ID       doc_id                                               text  \\\n",
       "0  62199  15449606311  Quality of material is great, however, the bac...   \n",
       "1  76123  15307152511  The product was okay but wasn't refined campho...   \n",
       "2  78742  12762748321  I normally read the reviews before buying some...   \n",
       "3  64010  15936405041  These pads are completely worthless. The light...   \n",
       "4  17058  13596875291  The saw works great but the blade oiler does n...   \n",
       "\n",
       "              date  star_rating  \\\n",
       "0   3/7/2018 19:47            3   \n",
       "1        43135.875            2   \n",
       "2      42997.37708            1   \n",
       "3      43313.25417            1   \n",
       "4  12/5/2017 20:17            2   \n",
       "\n",
       "                                               title  \n",
       "0                    great backpack with strange fit  \n",
       "1                                        Not refined  \n",
       "2                     Doesnt work, wouldnt recommend  \n",
       "3  The lighter colored side of the pads smells li...  \n",
       "4  The saw works great but the blade oiler does n...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>doc_id</th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "      <th>star_rating</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>62199</td>\n",
       "      <td>15449606311</td>\n",
       "      <td>Quality of material is great, however, the bac...</td>\n",
       "      <td>3/7/2018 19:47</td>\n",
       "      <td>3</td>\n",
       "      <td>great backpack with strange fit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>76123</td>\n",
       "      <td>15307152511</td>\n",
       "      <td>The product was okay but wasn't refined campho...</td>\n",
       "      <td>43135.875</td>\n",
       "      <td>2</td>\n",
       "      <td>Not refined</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>78742</td>\n",
       "      <td>12762748321</td>\n",
       "      <td>I normally read the reviews before buying some...</td>\n",
       "      <td>42997.37708</td>\n",
       "      <td>1</td>\n",
       "      <td>Doesnt work, wouldnt recommend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>64010</td>\n",
       "      <td>15936405041</td>\n",
       "      <td>These pads are completely worthless. The light...</td>\n",
       "      <td>43313.25417</td>\n",
       "      <td>1</td>\n",
       "      <td>The lighter colored side of the pads smells li...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17058</td>\n",
       "      <td>13596875291</td>\n",
       "      <td>The saw works great but the blade oiler does n...</td>\n",
       "      <td>12/5/2017 20:17</td>\n",
       "      <td>2</td>\n",
       "      <td>The saw works great but the blade oiler does n...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      ID       doc_id                                               text  \\\n",
       "0  62199  15449606311  Quality of material is great, however, the bac...   \n",
       "1  76123  15307152511  The product was okay but wasn't refined campho...   \n",
       "2  78742  12762748321  I normally read the reviews before buying some...   \n",
       "3  64010  15936405041  These pads are completely worthless. The light...   \n",
       "4  17058  13596875291  The saw works great but the blade oiler does n...   \n",
       "\n",
       "              date  star_rating  \\\n",
       "0   3/7/2018 19:47            3   \n",
       "1        43135.875            2   \n",
       "2      42997.37708            1   \n",
       "3      43313.25417            1   \n",
       "4  12/5/2017 20:17            2   \n",
       "\n",
       "                                               title  \n",
       "0                    great backpack with strange fit  \n",
       "1                                        Not refined  \n",
       "2                     Doesnt work, wouldnt recommend  \n",
       "3  The lighter colored side of the pads smells li...  \n",
       "4  The saw works great but the blade oiler does n...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.read_csv('../../data/final_project/test.csv', encoding='utf-8', header=0)\n",
    "\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Train a Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_text(train_df)\n",
    "concat_text(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID             0\n",
       "doc_id         0\n",
       "text           0\n",
       "date           0\n",
       "star_rating    0\n",
       "title          0\n",
       "human_tag      0\n",
       "concat         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "ID             0\n",
       "doc_id         0\n",
       "text           0\n",
       "date           0\n",
       "star_rating    0\n",
       "title          0\n",
       "human_tag      0\n",
       "concat         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.isna().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID             0\n",
       "doc_id         0\n",
       "text           0\n",
       "date           0\n",
       "star_rating    0\n",
       "title          0\n",
       "concat         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "ID             0\n",
       "doc_id         0\n",
       "text           0\n",
       "date           0\n",
       "star_rating    0\n",
       "title          0\n",
       "concat         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tesla T4'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'Tesla T4'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Implement this\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertConfig\n",
    "from pytorch_pretrained_bert import BertAdam, BertForSequenceClassification\n",
    "from tqdm import tqdm, trange\n",
    "import pandas as pd\n",
    "import io\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import torch\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "from transformers import Trainer, TrainingArguments, DistilBertForSequenceClassification, DistilBertTokenizerFast\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_metric\n",
    "\n",
    "# specify GPU device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_gpu = torch.cuda.device_count()\n",
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This separates 10% of the entire dataset into validation dataset.\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    train_df[\"concat\"].tolist(),\n",
    "    train_df[\"human_tag\"].tolist(),\n",
    "    test_size=0.3,\n",
    "    shuffle=True,\n",
    "    random_state=1234,\n",
    "    stratify = train_df[\"human_tag\"].tolist(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "train_encodings = tokenizer(train_texts,\n",
    "                            truncation=True,\n",
    "                            padding=True)\n",
    "val_encodings = tokenizer(val_texts,\n",
    "                          truncation=True,\n",
    "                          padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReviewDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]).to(device) for key, val in self.encodings.items()}\n",
    "        item[\"labels\"] = torch.tensor(self.labels[idx]).to(device)\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "train_dataset = ReviewDataset(train_encodings, train_labels)\n",
    "val_dataset = ReviewDataset(val_encodings, val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\",\n",
    "                                                            num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cbda8c29e98433fa864dad51e631f59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/2.06k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting training\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# Freeze the encoder weights until the classfier\n",
    "for name, param in model.named_parameters():\n",
    "    if \"classifier\" not in name:\n",
    "        param.requires_grad = False\n",
    "\n",
    "# Hyperparameters\n",
    "num_epochs = 10\n",
    "learning_rate=0.01\n",
    "\n",
    "# Get the compute device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Create data loaders\n",
    "train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=64, drop_last=True)\n",
    "eval_dataloader = DataLoader(val_dataset, batch_size=64, drop_last=True)\n",
    "\n",
    "# Setup the optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "metric = load_metric(\"f1\")\n",
    "model=model.to(device)\n",
    "print(\"starting training\")\n",
    "for epoch in range(num_epochs):\n",
    "    start = time.time()\n",
    "    training_loss = 0\n",
    "    val_loss = 0\n",
    "    print(epoch)\n",
    "    # Training loop starts\n",
    "    model.train() # put the model in training mode\n",
    "    i = 0\n",
    "    for batch in train_dataloader:\n",
    "        # below: ** allows us to pass multiple arguments to model()\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        training_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()   \n",
    "    # Validation loop starts\n",
    "    model.eval() # put the model in prediction mode\n",
    "    for batch in eval_dataloader:\n",
    "        with torch.no_grad():\n",
    "            # below:  ** allows us to pass multiple arguments to model()\n",
    "            outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        val_loss += loss.item()\n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "        metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "    # Let's take the average losses\n",
    "    training_loss = training_loss / len(train_dataloader)\n",
    "    val_loss = val_loss / len(eval_dataloader)\n",
    "    end = time.time()\n",
    "    \n",
    "    print(f\"Epoch {epoch}. Train_loss {training_loss:.4f}. Val_loss {val_loss:.4f}. \\\n",
    "    f1 {metric.compute()['f1']:.4f}. Seconds {end-start:.3f}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Make predictions on your test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_texts = test_df[\"concat\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_encodings = tokenizer(test_texts,\n",
    "                          truncation=True,\n",
    "                          padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = ReviewDataset(test_encodings, [0]*len(test_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_dataloader = DataLoader(test_dataset, batch_size=32)\n",
    "test_predictions = []\n",
    "model.eval()\n",
    "print(len(test_dataloader))\n",
    "i = 0\n",
    "for batch in test_dataloader:\n",
    "    if i % 10 == 0:\n",
    "        print(i)\n",
    "    i+=1\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**batch)\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    test_predictions.extend(predictions.cpu().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Write your predictions to a CSV file\n",
    "You can use the following code to write your test predictions to a CSV file. Then upload your file to https://mlu.corp.amazon.com/contests/redirect/53"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = pd.DataFrame()\n",
    "result_df[\"ID\"] = test_df[\"ID\"]\n",
    "result_df[\"human_tag\"] = test_predictions\n",
    " \n",
    "result_df.to_csv(\"../../data/final_project/project_day3_result.csv\", encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
